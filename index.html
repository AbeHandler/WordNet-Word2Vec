<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Wordnet-word2vec by AbeHandler</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Wordnet-word2vec</h1>
        <p>An empirical study of semantic similarity in WordNet and Word2Vec</p>

        <p class="view"><a href="https://github.com/AbeHandler/WordNet-Word2Vec">View the Project on GitHub <small>AbeHandler/WordNet-Word2Vec</small></a></p>


        <ul>
          <li><a href="https://github.com/AbeHandler/WordNet-Word2Vec/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/AbeHandler/WordNet-Word2Vec/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/AbeHandler/WordNet-Word2Vec">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h3>
<a name="an-empirical-study-of-semantic-similarity-in-wordnet-and-word2vec" class="anchor" href="#an-empirical-study-of-semantic-similarity-in-wordnet-and-word2vec"><span class="octicon octicon-link"></span></a>An empirical study of semantic similarity in WordNet and Word2Vec</h3>

<h5>
<a name="abram-handler--masters-thesis-university-of-new-orleans-fall-2014" class="anchor" href="#abram-handler--masters-thesis-university-of-new-orleans-fall-2014"><span class="octicon octicon-link"></span></a>Abram Handler | Master's Thesis: University of New Orleans, Fall 2014</h5>

<h4>
<a name="project" class="anchor" href="#project"><span class="octicon octicon-link"></span></a>Project</h4>

<p>Word2Vec is a new unsupervised system for determining the semantic distance between words. For instance, after learning from billions of web pages, Word2Vec reports that the words Chinese river are semantically close to the word Yangtze. Such results have attracted lots of recent attention: over 100 researchers have cited Word2Vec since its publication in 2013. Yet certain aspects of the system’s output are poorly understood. </p>

<p>In particular:</p>

<ol>
<li><p>Word2Vec does not label particular semantic relationships between words – like the synonomy between cold and chilly or the meronomy between wheel and car. Instead, it assigns a number between 0 and 1, indicating the semantic distance between two words. As Word2Vec's creator's note: “there can be many different types of similarities.” [2] This opens an unknon question: what sorts of semantic similarities does Word2Vec uncover?</p></li>
<li><p>Word2Vec can generate ranked lists showing which words are closer are further way in a semantic model. For example, Word2Vec says that grand-master is 3rd from the word chess, while Muay Thai kickboxing is 997th. What is the probability that two words that are k-apart in Word2Vec stand in some formal specific semantic relationship?</p></li>
</ol>

<p>This study seeks to answer such questions by comparing Word2Vec’s output with WordNet – a large, human-curated “lexical database” [3] and the most-frequently cited “lexiographic resource” [4] in English.</p>

<h4>
<a name="results" class="anchor" href="#results"><span class="octicon octicon-link"></span></a>Results</h4>

<p>Pick a word from the Reuters corpus. Get the 200 closest words in Word2Vec. How are these 200 words semantically related to the original word?</p>

<p><img src="images/total.png" alt="All results"></p>

<h4>
<a name="files" class="anchor" href="#files"><span class="octicon octicon-link"></span></a>Files</h4>

<p><code>main.sh</code> Primary driver for the application. Use ./main.sh to run the experiment and generate results</p>

<p><code>experiment.py</code> Runs the experiment</p>

<p><code>process_results.sh</code> Calculates totals used to show final results</p>

<p><code>tester.py</code> Some unit tests</p>

<p><code>wordnetchecker.py</code> Finds the average numbers of each relation in WordNet</p>

<h4>
<a name="dependencies" class="anchor" href="#dependencies"><span class="octicon octicon-link"></span></a>Dependencies</h4>

<ul>
<li><strong>NLTK 3.0.0</strong></li>
<li><strong>Gensim 0.10.2</strong></li>
<li><strong>numpy 1.9.0</strong></li>
<li>
<strong>Word vectors from Google news corpus</strong> <a href="https://code.google.com/p/word2vec/">downloadable here</a>.</li>
</ul>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/AbeHandler">AbeHandler</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>